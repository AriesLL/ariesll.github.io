[{"authors":null,"categories":null,"content":"","date":1671526391,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1671526391,"objectID":"01efc881200907c7f91948bb3e903b1d","permalink":"https://peipeizhou-eecs.github.io/author/jinming-zhuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinming-zhuang/","section":"authors","summary":"","tags":null,"title":"Jinming Zhuang","type":"authors"},{"authors":null,"categories":null,"content":"I joined the University of Pittsburgh, ECE department as a Tenure-Track Assistant Professor starting September 2021. I obtained my Ph.D. in Computer Science from University of California, Los Angeles in 2019 supervised by Prof. Jason Cong, who leads UCLA VAST(VLSI Architecture, Synthesis and Technology) Group and CDSC (The Center for Domain-Specific Computing). My major interest is in Customized Computer Architecture and Programming Abstraction for Applications including Healthcare, e.g., Precision Medicine and Artificial Intelligence. I\u0026rsquo;m honored to receive \u0026ldquo;Outstanding Recognition in Research\u0026rdquo; from UCLA Samueli School of Engineering in 2019. I have also received üèÜ 2019 TCAD Donald O. Pederson Best Paper Award üèÜ in recognition of best paper published in the IEEE Transactions on CAD in the two calendar years preceding the award. My paper has also received 2018 ICCAD Best Paper Nominee üèÜ, 2018 ISPASS Best Paper Nominee üèÜ.\nI\u0026rsquo;m actively recruiting PhD students and research interns! Self-motivated students with relevant research and project experience (compiler, GPU and FPGA programming, artificial intelligence algorithm and application development, etc.) are highly encouraged to contact me via email.  Download my CV. Former Website at UCLA\n","date":1671526391,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1671526391,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://peipeizhou-eecs.github.io/author/peipei-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/peipei-zhou/","section":"authors","summary":"I joined the University of Pittsburgh, ECE department as a Tenure-Track Assistant Professor starting September 2021. I obtained my Ph.D. in Computer Science from University of California, Los Angeles in 2019 supervised by Prof.","tags":null,"title":"Peipei Zhou","type":"authors"},{"authors":null,"categories":null,"content":"","date":1671526391,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1671526391,"objectID":"e47f45b6b981cbbfc6cba2768865b93d","permalink":"https://peipeizhou-eecs.github.io/author/zhuoping-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhuoping-yang/","section":"authors","summary":"","tags":null,"title":"Zhuoping Yang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d6c65411a5048b4b511d72fd42254ae1","permalink":"https://peipeizhou-eecs.github.io/author/kent-wirant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kent-wirant/","section":"authors","summary":"","tags":null,"title":"Kent Wirant","type":"authors"},{"authors":[],"categories":null,"content":"","date":1680886800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680886800,"objectID":"3a1dbfadbdfb45fcd389a6223a3fde32","permalink":"https://peipeizhou-eecs.github.io/talk/charm-composing-heterogeneous-accelerators-for-matrix-multiply-on-versal-acap-architecture/","publishdate":"2023-06-15T05:07:50.194Z","relpermalink":"/talk/charm-composing-heterogeneous-accelerators-for-matrix-multiply-on-versal-acap-architecture/","section":"event","summary":"Which platform beats 7nm GPU A100 in energy efficiency? AMD Versal ACAP (FPGA+AI Chip)\nHow to program AMD Versal ACAP, i.e., FPGA + AI Chip within the same chip die for deep learning applications in 10 lines of code? Use CHARM","tags":null,"title":"CHARM Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture","type":"event"},{"authors":null,"categories":null,"content":"Our¬†DAC¬†2023 submission,¬†\u0026ldquo;AutoMM: Energy-Efficient Multi-Data-Type Matrix Multiply Design on Heterogeneous Programmable System-on-Chip\u0026rdquo;¬†is accepted in¬†DAC 2023! Congratulations to my PhD students Jinming Zhuang and Zhuoping Yang!\n","date":1676316821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676316821,"objectID":"a4f5c22162bd6d64f54076905a16b9a8","permalink":"https://peipeizhou-eecs.github.io/post/02-13-2023-our-paper-accepted-to-dac-2023/","publishdate":"2023-02-13T19:33:41.341Z","relpermalink":"/post/02-13-2023-our-paper-accepted-to-dac-2023/","section":"post","summary":"Our¬†DAC¬†2023 submission,¬†\u0026ldquo;AutoMM: Energy-Efficient Multi-Data-Type Matrix Multiply Design on Heterogeneous Programmable System-on-Chip\u0026rdquo;¬†is accepted in¬†DAC 2023! Congratulations to my PhD students Jinming Zhuang and Zhuoping Yang!","tags":null,"title":"02/13/2023 Our Paper Accepted to DAC 2023!","type":"post"},{"authors":["Jinming Zhuang","Jason Lau","Hanchen Ye","Zhuoping Yang","Yubo Du","Jack Lo","Kristof Denolf","Stephen Neuendorffer","Alex K. Jones","Jingtong Hu","Deming Chen","Jason Cong","Peipei Zhou"],"categories":null,"content":"","date":1671526391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671526391,"objectID":"f704c7ce18051c6e76da0a657cecdcda","permalink":"https://peipeizhou-eecs.github.io/publication/fpga23/","publishdate":"2022-12-20T08:53:11.299Z","relpermalink":"/publication/fpga23/","section":"publication","summary":"","tags":null,"title":"CHARM: Composing Heterogeneous AcceleRators for Matrix Multiply on Versal ACAP Architecture","type":"publication"},{"authors":["S√©bastien Ollivier","Sheng Li","Yue Tang","Chayanika Chaudhuri","Peipei Zhou","Xulong Tang","Jingtong Hu","Alex K. Jones"],"categories":null,"content":"","date":1671441152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671441152,"objectID":"ad6ffc9478391c6507624f41a46afc5a","permalink":"https://peipeizhou-eecs.github.io/publication/2022_ieee_micro/","publishdate":"2022-12-19T09:12:32.506Z","relpermalink":"/publication/2022_ieee_micro/","section":"publication","summary":"","tags":["IEEE Micro"],"title":"Sustainable AI Processing at the Edge","type":"publication"},{"authors":null,"categories":null,"content":"Our¬†FPGA¬†2023 submission,¬†CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture¬†is accepted in¬†FPGA¬†2023! Congratulations to my PhD student Jinming Zhuang on the first-author publication! TÔªøhis is a collaborated work with UIUC, UCLA, AMD/Xilinx. Huge thanks to all the collaborators!\nTÔªøhe preprint will be released soon! The open-source project will be released soon!\n","date":1670908938,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670908938,"objectID":"dc360ed61194c507449f08dc18005e4f","permalink":"https://peipeizhou-eecs.github.io/post/fpga23-paper-accepted/","publishdate":"2022-12-13T05:22:18.139Z","relpermalink":"/post/fpga23-paper-accepted/","section":"post","summary":"Our¬†FPGA¬†2023 submission,¬†CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture¬†is accepted in¬†FPGA¬†2023! Congratulations to my PhD student Jinming Zhuang on the first-author publication!","tags":null,"title":"FPGA23 Paper Accepted!!","type":"post"},{"authors":null,"categories":null,"content":"Prof. Zhou will attend DAC 2022 taking place this July 10-14 at the Moscone Center West in San Francisco, CA. Prof. Zhou will serve as the¬†Session Chair¬†for¬†EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!\u0026rdquo; on Tuesday July 12th 1:30pm to 3pm PST at¬†3007. Prof. Zhou will also serve as vice co-chair for University Demonstration at DAC 2022. University¬†Demo¬†will be held on¬†Tuesday July 12th 7pm to 9pm PST at¬†Level 3 Lobby.\nlink: https://59dac.conference-program.com/\n","date":1657267055,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657267055,"objectID":"2276e06b3bba8eec6ba6136fff4d1a9e","permalink":"https://peipeizhou-eecs.github.io/post/2022-07-08-prof-zhou-will-serve-as-session-chair-for-dac-2022-eda-system-on-chip-design-methodology-san-francisco/","publishdate":"2022-07-08T07:57:35.732Z","relpermalink":"/post/2022-07-08-prof-zhou-will-serve-as-session-chair-for-dac-2022-eda-system-on-chip-design-methodology-san-francisco/","section":"post","summary":"Prof. Zhou will attend DAC 2022 taking place this July 10-14 at the Moscone Center West in San Francisco, CA. Prof. Zhou will serve as the¬†Session Chair¬†for¬†EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!","tags":["conference"],"title":"2022/07/08 Prof. Zhou will serve as session chair for DAC 2022 EDA System-on-Chip Design Methodology, San Francisco","type":"post"},{"authors":null,"categories":null,"content":"Prof. Zhou will serve as TPC for 2022 International Conference on Computer-Aided Design, 2023 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design \u0026amp; Test , 2023 IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE)\n","date":1657265804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657265804,"objectID":"6885664022fe4b52bb9779ce23df4538","permalink":"https://peipeizhou-eecs.github.io/post/prof-zhou-will-serve-as-technical-program-committee-tpc-for-2022-iccad-2023-date-2023-chase/","publishdate":"2022-07-08T07:36:44.473Z","relpermalink":"/post/prof-zhou-will-serve-as-technical-program-committee-tpc-for-2022-iccad-2023-date-2023-chase/","section":"post","summary":"Prof. Zhou will serve as TPC for 2022 International Conference on Computer-Aided Design, 2023 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design \u0026amp; Test , 2023 IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE)","tags":null,"title":"2022/07/08 Prof. Zhou will serve as technical program committee (TPC) for 2022 ICCAD, 2023 DATE, 2023 CHASE","type":"post"},{"authors":[],"categories":null,"content":"Welcome Zhuoping! Zhuoping will join the lab in August 2022.\nZhuoping graduated with Bachelor of Engineering in Electronic Packaging and Master of Engineering in Electronic Packaging from Huazhong University of Science and Technology.\n","date":1657264892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657264892,"objectID":"4ab1ea3f108cf3cc8504583ffb3a7d72","permalink":"https://peipeizhou-eecs.github.io/post/welcome-zhuoping-yang-to-join-our-lab/","publishdate":"2022-07-08T07:21:32.312Z","relpermalink":"/post/welcome-zhuoping-yang-to-join-our-lab/","section":"post","summary":"Welcome Zhuoping! Zhuoping will join the lab in August 2022.\nZhuoping graduated with Bachelor of Engineering in Electronic Packaging and Master of Engineering in Electronic Packaging from Huazhong University of Science and Technology.","tags":["recruiting"],"title":"2022/07/08 Welcome Zhuoping Yang to join our lab!","type":"post"},{"authors":["Xinyi Zhang","Cong Hao","Peipei Zhou","Alex Jones","Jingtong Hu"],"categories":null,"content":"","date":1646449993,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646449993,"objectID":"28385e95cf8f298676c5f80035721cee","permalink":"https://peipeizhou-eecs.github.io/publication/2022_dac/","publishdate":"2022-03-05T03:13:13.544Z","relpermalink":"/publication/2022_dac/","section":"publication","summary":"The complex nature of real-world problems calls for heterogeneity in both machine learning (ML) models and hardware systems. For the algorithm, the heterogeneity in ML models comes from the multi-sensor perceiving and multi-task learning, i.e., multi-modality multi-task (MMMT) models, resulting in diverse deep neural net- work (DNN) layers and computation patterns. For the system, it becomes prevailing to integrate dedicated acceleration components into one system. It thus introduces a new problem, heterogeneous model to heterogeneous system mapping (H2H), in which both computation and communication efficiency need to be considered. While previous mapping algorithms only focus on computation patterns, in this work, we propose a novel mapping algorithm with both computation and communication awareness. By slightly sacrificing computation efficiency, the communication latency is largely reduced. Therefore, the system overall performance is improved and energy is also reduced. The superior performance of our work is evaluated on MAESTRO, achieving 15%-74% latency improvement and 23%-64% energy reduction when compared with the existing computation-prioritized mapping algorithm.","tags":["DAC"],"title":"H2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness","type":"publication"},{"authors":["Yue Tang","Xinyi Zhang","Peipei Zhou","Jingtong Hu"],"categories":null,"content":"","date":1645776277,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645776277,"objectID":"53c9ee3a10d5e893a328b9a4d76e2f05","permalink":"https://peipeizhou-eecs.github.io/publication/2022_todaes/","publishdate":"2022-02-25T08:04:37.024Z","relpermalink":"/publication/2022_todaes/","section":"publication","summary":"Conventionally, DNN models are trained once in the cloud and deployed in edge devices such as cars, robots, or unmanned aerial vehicles (UAVs) for real-time inference. However, there are many cases that require the models to adapt to new environments, domains, or new users. In order to realize such domain adaption or personalization, the models on devices need to be continuously trained on the device. In this work, we design EF-Train, an efficient DNN training accelerator with a unified channel-level parallelism-based convolution kernel that can achieve end-to-end training on resource-limited low-power edge-level FPGAs. It is challenging to implement on-device training on resource-limited FPGAs due to the low efficiency caused by different memory access patterns among forward, backward propagation, and weight update. Therefore, we developed a data reshaping approach with intra-tile continuous memory allocation and weight reuse. An analytical model is established to automatically schedule computation and memory resources to achieve high energy efficiency on edge FPGAs. The experimental results show that our design achieves 46.99 GFLOPS and 6.09GFLOPS/W in terms of throughput and energy efficiency, respectively.","tags":["TODAES"],"title":"EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization","type":"publication"},{"authors":null,"categories":null,"content":"One paper \u0026ldquo;EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization\u0026rdquo; got accepted in ACM Transactions on Design Automation of Embedded Systems (TODAES) Special Issue on Energy-Efficient AI Chips\n","date":1645170217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645170217,"objectID":"9177697ac2258e99411943ef0748411f","permalink":"https://peipeizhou-eecs.github.io/post/one-paper-accepted-in-todaes/","publishdate":"2022-02-18T07:43:37.768Z","relpermalink":"/post/one-paper-accepted-in-todaes/","section":"post","summary":"One paper \u0026ldquo;EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization\u0026rdquo; got accepted in ACM Transactions on Design Automation of Embedded Systems (TODAES) Special Issue on Energy-Efficient AI Chips","tags":null,"title":"One Paper Accepted in TODAES","type":"post"},{"authors":null,"categories":null,"content":"One paper ‚ÄúH2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness‚Äù (Xinyi Zhang et al.) got accepted by¬†DAC‚Äô22.\n","date":1645170110,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645170110,"objectID":"b66e316eedf92340441258994ac66a49","permalink":"https://peipeizhou-eecs.github.io/post/one-paper-accepted-in-dac22/","publishdate":"2022-02-18T07:41:50.534Z","relpermalink":"/post/one-paper-accepted-in-dac22/","section":"post","summary":"One paper ‚ÄúH2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness‚Äù (Xinyi Zhang et al.) got accepted by¬†DAC‚Äô22.","tags":null,"title":"One Paper Accepted in DAC'22","type":"post"},{"authors":null,"categories":null,"content":"Welcome Jinming and Yubo to join our lab! Jinming joined the lab in September 2021 and Yubo will join the lab in January 2022.\n","date":1638424782,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638424782,"objectID":"433d7d3dee510b759d32eebad50eb25b","permalink":"https://peipeizhou-eecs.github.io/post/welcome-jinming-zhuang-and-yubo-du-to-join-our-lab/","publishdate":"2021-12-02T05:59:42.458Z","relpermalink":"/post/welcome-jinming-zhuang-and-yubo-du-to-join-our-lab/","section":"post","summary":"Welcome Jinming and Yubo to join our lab! Jinming joined the lab in September 2021 and Yubo will join the lab in January 2022.","tags":["recruiting"],"title":"Welcome Jinming Zhuang and Yubo Du to join our lab!","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou will serve as session chair for Session: Data Collection and Analysis at IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE 2021), December 16 - 18, 2021 Washington D.C., USA.\nhttps://conferences.computer.org/chase2021/program.html\n","date":1637731481,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637731481,"objectID":"11e9510c76099c62307693821cab3507","permalink":"https://peipeizhou-eecs.github.io/post/dr-zhou-will-serve-as-session-chair-for-chase-2021/","publishdate":"2021-11-24T05:24:41.06Z","relpermalink":"/post/dr-zhou-will-serve-as-session-chair-for-chase-2021/","section":"post","summary":"Dr. Zhou will serve as session chair for Session: Data Collection and Analysis at IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE 2021), December 16 - 18, 2021 Washington D.","tags":null,"title":"11/24/2021 Dr. Zhou will serve as session chair for CHASE 2021","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou will serve on TPC for Fifth Conference on Machine Learning and Systems (MLSys 2022), Apr. 11th to 14th, 2022 at Santa Clara Convention Center.\n","date":1637730428,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637730428,"objectID":"089afd8baf23822037083b2e878c2a2e","permalink":"https://peipeizhou-eecs.github.io/post/11-24-2021-dr-zhou-will-serve-on-tpc-for-mlsys-2022/","publishdate":"2021-11-24T05:07:08.178Z","relpermalink":"/post/11-24-2021-dr-zhou-will-serve-on-tpc-for-mlsys-2022/","section":"post","summary":"Dr. Zhou will serve on TPC for Fifth Conference on Machine Learning and Systems (MLSys 2022), Apr. 11th to 14th, 2022 at Santa Clara Convention Center.","tags":null,"title":"11/24/2021 Dr. Zhou will serve on TPC for MLSys 2022","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou is invited to serve on Technique Program Committee for The 30th IEEE International Symposium On Field-Programmable Custom Computing Machines (FCCM 2022), May 15 ‚Äî May 18, 2022, New York.\nCFP: https://www.fccm.org/call-for-papers/.\n","date":1637730097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637730097,"objectID":"fc420df3a42f47d5fd440a2913a52540","permalink":"https://peipeizhou-eecs.github.io/post/11-23-2021-dr-zhou-will-serve-on-tpc-for-fccm-2022/","publishdate":"2021-11-24T05:01:37.29Z","relpermalink":"/post/11-23-2021-dr-zhou-will-serve-on-tpc-for-fccm-2022/","section":"post","summary":"Dr. Zhou is invited to serve on Technique Program Committee for The 30th IEEE International Symposium On Field-Programmable Custom Computing Machines (FCCM 2022), May 15 ‚Äî May 18, 2022, New York.","tags":null,"title":"11/23/2021 Dr. Zhou will serve on TPC for FCCM 2022","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou will attend DAC 2021 Dec 5th to Dec 9th 2021 and serve as the Session Chair for EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!\u0026quot;\n","date":1637729246,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637729246,"objectID":"cff2647fd7c9b361e64fd4139001b79a","permalink":"https://peipeizhou-eecs.github.io/post/peipei-zhou-will-serve-as-session-chair-for-dac-2021-eda-system-on-chip-design/","publishdate":"2021-11-24T04:47:26.304Z","relpermalink":"/post/peipei-zhou-will-serve-as-session-chair-for-dac-2021-eda-system-on-chip-design/","section":"post","summary":"Dr. Zhou will attend DAC 2021 Dec 5th to Dec 9th 2021 and serve as the Session Chair for EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!\u0026quot;","tags":null,"title":"11/22/2021 Peipei Zhou will serve as session chair for DAC 2021: EDA System-on-Chip Design","type":"post"},{"authors":["Xinyi Zhang","Yawen Wu","Peipei Zhou","Xulong Tang","Jingtong Hu"],"categories":null,"content":"","date":1632933408,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632933408,"objectID":"f8efb5979ddb735398ec397e6655d874","permalink":"https://peipeizhou-eecs.github.io/publication/tecs2021/","publishdate":"2021-09-29T16:36:48.458Z","relpermalink":"/publication/tecs2021/","section":"publication","summary":"Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA.","tags":["TECS","ESWEEK"],"title":"Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices","type":"publication"},{"authors":["Peipei Zhou","Jiayi Sheng","Cody Hao Yu","Peng Wei","Jie Wang","Di Wu","Jason Cong"],"categories":null,"content":"","date":1620984449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620984449,"objectID":"6fe94c89edf32709a2c92e29b56e496f","permalink":"https://peipeizhou-eecs.github.io/publication/mocha/","publishdate":"2021-05-14T09:27:29.087Z","relpermalink":"/publication/mocha/","section":"publication","summary":"FPGAs have been widely deployed in public clouds, e.g., Amazon Web Services (AWS) and Huawei Cloud. However, simply offloading accelerated kernels from CPU hosts to PCIe-based FPGAs does not guarantee out-of-pocket cost savings in a pay-as-you-go public cloud. Taking Genome Analysis Toolkit (GATK) applications as case studies, although the adoption of FPGAs reduces the overall execution time, it introduces 2.56√ó extra cost, due to insufficient application-level speedup by Amdahl‚Äôs law. To optimize the out-of-pocket cost while keeping high speedup and throughput, we propose Mocha framework as a distributed runtime system to fully utilize the accelerator resource by accelerator sharing and CPU-FPGA partial task offloading. Evaluation results on HaplotypeCaller (HTC) and Mutect2 in GATK show that on AWS, Mocha saves on the application cost by 2.82x for HTC, 1.06x for Mutect2 and on Huawei Cloud by 1.22x, 1.52x respectively than straightforward CPU-FPGA integration solution with less than 5.1% performance overhead.","tags":["FPGA"],"title":"MOCHA: Multinode Cost Optimization in Heterogeneous Clouds with Accelerators","type":"publication"},{"authors":["Michael Lo","Zhenman Fang","Jie Wang","Peipei Zhou","Mau-Chung Frank Chang","Jason Cong"],"categories":null,"content":"","date":1588318259,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588318259,"objectID":"b105445cfb4ab0be802768433894d8be","permalink":"https://peipeizhou-eecs.github.io/publication/algorithm-hardware-co-design-for-bqsr-acceleration-in-genome-analysis-toolkit/","publishdate":"2020-05-01T07:30:59.733Z","relpermalink":"/publication/algorithm-hardware-co-design-for-bqsr-acceleration-in-genome-analysis-toolkit/","section":"publication","summary":"Genome sequencing is one of the most important applications in healthcare and has a great potential to realize precision medicine and personalized healthcare. However, its computing process is very time consuming. Even pre-processing the raw sequence data of a whole genome for a single person to the analysis ready data can take several days on a single-core CPU.\n\nIn this paper, we propose to accelerate the performance of the widely used Genome Analysis ToolKit (GATK) using FPGAs. More specifically, we focus on the algorithm and hardware codesign for the Base Quality Score Re-calibration (BQSR) step in GATK, which is an important and time-consuming step to correct systematic errors made by a sequencing machine. Prior studies did not consider hardware acceleration for BQSR because it requires a large amount of memory with random access and has a lot of control flow. To address these challenges, we first adapt the algorithm to resolve the random memory access conflicts to achieve a fully pipelined accelerator design and reduce its dataset size. Second, we leverage the newly introduced large-capacity UltraRAM (URAM) in Xilinx UltraScale+ FPGAs to buffer BQSR‚Äôs large dataset on chip, and further optimize its operating frequency. Finally, we also explore the coarse-grained pipeline and parallelism to improve the overall performance of the BQSR accelerator. Compared to the latest software implementation of GATK 4.1 running on single-thread and 56-thread CPUs (14nm Xeon E5-2680 v4), our FPGA accelerator running on Xilinx 16nm UltraScale+ VCU1525 board achieves up to 40.7x and 8.5x speedups, respectively.","tags":["FCCM"],"title":"Algorithm-Hardware Co-design for BQSR Acceleration in Genome Analysis ToolKit","type":"publication"},{"authors":["Peipei Zhou"],"categories":null,"content":"","date":1560165499,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560165499,"objectID":"126abc48a4a1b5e5269b8f38857877be","permalink":"https://peipeizhou-eecs.github.io/publication/2019_phd/","publishdate":"2019-06-10T11:18:19.911Z","relpermalink":"/publication/2019_phd/","section":"publication","summary":"This dissertation investigates design target, modeling, and optimization for field-programmable gate array (FPGA) customized computing at chip-level, node-level and cluster-level. FPGAs have gained popularity in the acceleration of a wide range of applications with 10x-100x performance/energy efficiency over the general-purpose processors. The design choices of FPGA accelerators for different targets at different levels are enormous. To guide the designers to find the best design choices, modeling is inevitable. Chip-level performance and energy modeling for embedded and low-power devices. We first study the single chip performance and energy model for FPGA-based pipelined design. Customized pipeline designs that minimize the pipeline initiation interval (II) maximize the throughput of FPGA accelerators designed with high-level synthesis (HLS). However, II1 can reduce dynamic energy below II=1 due to interconnect savings. We use analytic models to describe accelerator performance and energy, explore the trade-offs of energy and accelerator performance. and find the energy optimal design point. Chip-level performance and frequency improvement through locality-aware transformation in HLS. We then study timing degradation problems in HLS-based accelerator design and classify four patterns: scatter, gather, broadcast, and reduce in the context of on-chip data movement. We observe that the on-chip data path delay in these patterns scales up when the design size increases, but HLS tools do not estimate the interconnect delay correctly or make a conscientious effort to control or cap the growth of long interconnect delays at the HLS level. We propose a Latte microarchitecture that features pipelined transfer controllers (PTC) to reduce critical path and improves timing by 1.50x on average. Node-level performance and cost modeling for FPGA-enabled, storage-optimized public cloud instances. At node level, We study performance and cost models for customized computing in light of the fact that performance and cost are primary concerns when deploying applications and services in a pay-as-you-go public cloud. The performance and cost modeling are discussed in two aspects, computation resources, with CPUs and locally PCIe-attached accelerators, and storage resources including SSDs and HDDs. For computation resources, improved performance using accelerators is accompanied by a higher cost per hour. We discuss the performance and cost modeling of deploying FPGA accelerators, offer insights on accelerator kernel design, and discuss when we should scale up by using FPGA in a node or by choosing a larger instance which has more CPU cores per node. For storage resources, storage systems (SSD/HDD) need to be carefully chosen to match the performance improvement introduced by accelerators while achieving the optimal cost. We conduct quantitative performance analysis on the Spark-based production-quality genome analysis toolkit. We then propose I/O-aware performance analysis and modeling for a broad set of Spark applications. Based on the model, we optimize the cost of genome sequencing in the public cloud by 38%, compared to a configuration recommended by the Spark Official website. Cluster-level performance and cost modeling for sharing FPGAs among different instances. From a node-level performance and cost model, we learn that simply offloading accelerated kernels from CPU hosts to PCIe-based FPGAs does not guarantee improvement in terms of out-of-pocket cost when using pay-as-you-go services in a public cloud. We analyze the application execution and conclude that the extra cost is attributable to insufficient application-level speedup by Amdahl‚Äôs law. To achieve cost saving with the use of FPGA accelerators in the public cloud, we propose to share one FPGA among multiple CPU instances when the number of CPU cores in one instance cannot fully utilize the FPGA accelerator computation resource. By implementing this idea, we present Mocha framework in this dissertation as a distributed runtime system to optimize the out-of-pocket cost while keeping high speedup and throughput. To demonstrate the performance improvement and cost saving of modeling in customized computing, we use genome pipeline optimization in the public cloud and private cloud as case studies showing how to conduct optimal scheduling under certain constraints. In the public cloud, where cost is the primary concern, we formulate how to select instances and schedule genome stages to achieve the least cost given certain deadline constraints as a MILP (mixed integer linear programming) problem. In a private cloud, where hardware (CPU cores, storage disks) is given, we formulate the scheduling of multiple genomes to achieve the least latency, as a MILP problem.","tags":["Thesis"],"title":"Modeling and Optimization for Customized Computing: Performance, Energy and Cost Perspective","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://peipeizhou-eecs.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Jason Cong","Peipei Zhou"],"categories":null,"content":"","date":1544444034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544444034,"objectID":"7faed76bbfac0ea5c18f095d72a2b030","permalink":"https://peipeizhou-eecs.github.io/talk/customizable-domain-specific-computing/","publishdate":"2021-08-17T12:13:54.603Z","relpermalink":"/talk/customizable-domain-specific-computing/","section":"event","summary":"","tags":null,"title":"Customizable Domain Specific Computing","type":"event"},{"authors":["Chen Zhang","Guangyu Sun","Zhenman Fang","Peipei Zhou","Peichen Pan","Jason Cong"],"categories":null,"content":"","date":1542453237,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542453237,"objectID":"9a8adcaf1ff46aee8d1f93b9277d279f","permalink":"https://peipeizhou-eecs.github.io/publication/2018_tcad/","publishdate":"2018-11-17T11:13:57.518Z","relpermalink":"/publication/2018_tcad/","section":"publication","summary":"With the recent advancement of multilayer convolutional neural networks (CNN) and fully connected networks (FCN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy efficiency of the computation-demanding CNN, the FPGAbased acceleration emerges as one of the most attractive alternatives. In this paper we design and implement Caffeine, a hardware/ software co-designed library to efficiently accelerate the entire CNN and FCN on FPGAs. First, we propose a uniformed convolutional matrixmultiplication representation for both computation-bound convolutional layers and communication-bound fully connected (FCN) layers. Based on this representation, we optimize the accelerator micro-architecture and maximize the underlying FPGA computing and bandwidth resource utilization based on a revised roofline model. Moreover, we design an automation flow to directly compile high-level network definitions to the final FPGA accelerator. As a case study, we integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet networks on multiple FPGA platforms. Caffeine achieves a peak performance of 1,460 GOPS on a medium-sized Xilinx KU060 FPGA board; to our knowledge, this is the best published result. It achieves more than 100x speed-up on FCN layers over prior FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 29x and 150x performance and energy gains over Caffe on a 12-core Xeon server, and 5.7x better energy efficiency over the GPU implementation. Performance projections for a system with a high-end FPGA (Virtex7 690t) show even higher gains.","tags":["TCAD"],"title":"Caffeine: Towards Uniformed Representation and Acceleration for Deep Convolutional Neural Networks (üî•Best Paper)","type":"publication"},{"authors":["Yuze Chi","Jason Cong","Peng Wei","Peipei Zhou"],"categories":null,"content":"","date":1542444246,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542444246,"objectID":"f629e0aebdc547b4513b8e28e5851369","permalink":"https://peipeizhou-eecs.github.io/publication/2018_iccad/","publishdate":"2018-11-17T08:44:06.201Z","relpermalink":"/publication/2018_iccad/","section":"publication","summary":"Stencil computation is one of the most important kernels in many application domains such as image processing, solving partial diferential equations, and cellular automata. Many of the stencil kernels are complex, usually consist of multiple stages or iterations, and are often computation-bounded. Such kernels are often off-loaded to FPGAs to take advantages of the efficiency of dedicated hardware. However, implementing such complex kernels efficiently is not trivial, due to complicated data dependencies, difficulties of programming FPGAs with RTL, as well as large design space.\nIn this paper we present SODA, an automated framework for implementing Stencil algorithms with Optimized Datalow Architecture on FPGAs. The SODA microarchitecture minimizes the on-chip reuse bufer size required by full data reuse and provides flexible and scalable fine-grained parallelism. The SODA automation framework takes high-level user input and generates efficient, high-frequency datalow implementation. This significantly reduces the difficulty of programming FPGAs efficiently for stencil algorithms. The SODA design-space exploration framework models the resource constraints and searches for the performance-optimized coniguration with accurate models for post-synthesis resource utilization and on-board execution throughput. Experimental results from on-board execution using a wide range of benchmarks show up to 3.28x speed up over 24-thread CPU and our fully automated framework achieves better performance compared with manually designed state-of-the-art FPGA accelerators.","tags":["ICCAD"],"title":"SODA: Stencil with Optimized Dataflow Architecture (üî•Best Paper Nominee)","type":"publication"},{"authors":["Jason Cong","Zhenman Fang","Yuchen Hao","Peng Wei","Cody Hao Yu","Chen Zhang","Peipei Zhou"],"categories":null,"content":"","date":1534504545,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534504545,"objectID":"1835eafa9e829c1c9391c0ab5c2adfc9","permalink":"https://peipeizhou-eecs.github.io/publication/2018_arxiv/","publishdate":"2018-08-17T11:15:45.333Z","relpermalink":"/publication/2018_arxiv/","section":"publication","summary":"FPGA-based heterogeneous architectures provide programmers with the ability to customize their hardware accelerators for flexible acceleration of many workloads. Nonetheless, such advantages come at the cost of sacrificing programmability. FPGA vendors and researchers attempt to improve the programmability through high-level synthesis (HLS) technologies that can directly generate hardware circuits from high-level language descriptions. However, reading through recent publications on FPGA designs using HLS, one often gets the impression that FPGA programming is still hard in that it leaves programmers to explore a very large design space with many possible combinations of HLS optimization strategies. In this paper we make two important observations and contributions. First, we demonstrate a rather surprising result: FPGA programming can be made easy by following a simple best-effort guideline of five refinement steps using HLS. We show that for a broad class of accelerator benchmarks from MachSuite, the proposed best-effort guideline improves the FPGA accelerator performance by 42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator performance is improved from an average 292.5x slowdown to an average 34.4x speedup. Moreover, we show that the refinement steps in the best-effort guideline, consisting of explicit data caching, customized pipelining, processing element duplication, computation/communication overlapping and scratchpad reorganization, correspond well to the best practice guidelines for multicore CPU programming. Although our best-effort guideline may not always lead to the optimal solution, it substantially simplifies the FPGA programming effort, and will greatly support the wide adoption of FPGA-based acceleration by the software programming community.","tags":["arXiv"],"title":"Best-effort FPGA programming: a few steps can go a long way","type":"publication"},{"authors":["Peipei Zhou","Zhenyuan Ruan","Zhenman Fang","Megan Shand","David Roazen","Jason Cong"],"categories":null,"content":"","date":1526641566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526641566,"objectID":"94e18cfcb9e3c8806ad5a0e003c4a5e9","permalink":"https://peipeizhou-eecs.github.io/publication/2018_ispass/","publishdate":"2018-05-18T11:06:06.481Z","relpermalink":"/publication/2018_ispass/","section":"publication","summary":"In conventional Hadoop MapReduce applications, I/O used to play a heavy role in the overall system performance. More recently, a study from the Apache Spark community‚Äîstate-of-the-art in-memory cluster computing framework‚Äîreports that I/O is no longer the bottleneck and has a marginal performance impact on applications like SQL processing. However, we observe that simply replacing HDDs with SSDs in a Spark cluster can have over 10x performance improvement for certain stages in large-scale production-quality genome processing. Therefore, one key question arises: How does I/O quantitatively impact the performance of today‚Äôs big data applications developed using in-memory cluster computing frameworks like Apache Spark? In this paper we select an important yet complex application‚Äîthe Spark-based Genome Analysis ToolKit (GATK4)‚Äîto guide our modeling. We first use different combinations of HDDs and SSDs to measure the I/O impact on GATK4 and change the CPU core number to discover the relation between computation and I/O access. Combining with Spark underlying implementations, we further analyze the inherent cause of the above observations and build our model based on the analysis. Although building upon GATK4, our model maintains generality to other applications. Experimental results show that we can achieve an performance prediction error rate within 10% for typical Spark applications of both iterative and shuffle-heavy algorithms. Finally, we further extend our model to a broader area - that of optimal configuration selection in the public cloud. In Google Cloud, our model enables us to save 38% to 57% cost for genome sequencing compared with its recommended default configurations. Currently, more and more companies are adopting cloud computing for specific workloads. Our proposed model can have a huge impact on their choices, while also enabling them to significantly reduce their costs.","tags":["ISPASS"],"title":"Doppio: I/O-Aware Performance Analysis, Modeling and Optimization for In-Memory Computing Framework(üî•Best Paper Nominee)","type":"publication"},{"authors":["Jason Cong","Peng Wei","Cody Hao Yu","Peipei Zhou"],"categories":null,"content":"","date":1526548483,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526548483,"objectID":"9195139fb3f77652cda4d3cfa204de1e","permalink":"https://peipeizhou-eecs.github.io/publication/2018_fccm_latte/","publishdate":"2018-05-17T09:14:43.992Z","relpermalink":"/publication/2018_fccm_latte/","section":"publication","summary":"Modern FPGA chips feature abundant reconfigurable resources such as LUTs, FFs, BRAMs and DSPs. High-level synthesis (HLS) further advances users productivity in designing accelerators and scaling out the design quickly via fine-grain and coarse-grain pipelining and duplication to utilize on-chip resources. However, current HLS tools fail to consider data locality in the scaled-out design; this leads to a long critical path which results in a low operating frequency. In this paper we summarize the timing degradation problems to four common collective communication and computation patterns in HLS-based accelerator design: scatter, gather, broadcast and reduce. These widely used patterns scale poorly in one-to-all or all-to-one data movements between off-chip communication interface and on-chip storage, or inside the computation logic. Therefore, we propose the Latte microarchitecture featuring pipelined transfer controllers (PTC) along data paths in these patterns. Furthermore, we implement an automated framework to apply our Latte implementation in HLS with minimal user efforts. Our experiments show that Latte-optimized designs greatly improve the timing of baseline HLS designs by 1.50x with only 3.2% LUT overhead on average, and 2.66x with 2.7% overhead at maximum.","tags":["FCCM","HLS"],"title":"Latte: Locality Aware Transformation for High-Level Synthesis","type":"publication"},{"authors":["Zhenyuan Ruan","Tong He","Bojie Li","Peipei Zhou","Jason Cong"],"categories":null,"content":"","date":1526468642,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526468642,"objectID":"74cfa225d4a4adc3dcea7fb22d36f66c","permalink":"https://peipeizhou-eecs.github.io/publication/2018_fccm_staccel/","publishdate":"2018-05-16T11:04:02.914Z","relpermalink":"/publication/2018_fccm_staccel/","section":"publication","summary":"In recent years we have witnessed the emergence of the FPGA in many high-performance systems. This is due to FPGA's high reconfigurability and improved user-friendly programming environment. OpenCL, supported by major FPGA vendors, is a high-level programming platform that liberates hardware developers from having to deal with the complex and error-prone HDL development. While OpenCL exposes a GPU-like programming model, which is well-suited for compute-intensive tasks, in many state-of-art systems that deploy FPGA, we observe that the workloads are streaming-like, which is communication-intensive. This mismatch leads to low throughput and high end-to-end latency.\nIn this paper, we propose ST-Accel, a new high-level programming platform for streaming applications on FPGA. It has the following advantages: (i) ST-Accel adopts the multiprocessing programming model to capture the inherent pipeline-level parallelism of streaming applications while reducing the end-to-end latency. (ii) A message-passing-based host/FPGA communication model is used to avoid the coherency issue of shared memory, thus enabling host/FPGA communication during kernel execution. (iii) ST-Accel provides a high-level abstraction for I/O devices to support direct I/O device access that eliminates the overhead of host CPU and reduces the I/O latency. (iv) ST-Accel enables the decoupled access/execute architecture to maximize the utilization of I/O devices. (v) The host/FPGA communication interface is redesigned to cater to the demands of both latency-critical and throughput-critical scenarios. The experimental results on the Amazon AWS cloud and local machine show that ST-Accel can achieve 1.6X-166X throughput and 1/3 latency for typical streaming workloads when compared to OpenCL.","tags":["FCCM"],"title":"ST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA","type":"publication"},{"authors":["Jason Cong","Peng Wei","Cody Hao Yu","Peipei Zhou"],"categories":null,"content":"","date":1497697632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497697632,"objectID":"6f2042b6da9c691ff552e3eb3c4924d5","permalink":"https://peipeizhou-eecs.github.io/publication/2017_dac/","publishdate":"2017-06-17T11:07:12.497Z","relpermalink":"/publication/2017_dac/","section":"publication","summary":"High-level synthesis (HLS) is getting increasing attention from both academia and industry for high-quality and high-productivity designs. However, when inferring primitive-type arrays in HLS designs into on-chip memory buffers, commercial HLS tools fail to effectively organize FPGAs‚Äô on-chip BRAM building blocks to realize high-bandwidth data communication; this often leads to suboptimal quality of results. This paper addresses this issue via automated on-chip buffer restructuring. Specifically, we present three buffer restructuring approaches and develop an analytical model for each approach to capture its impact on performance and resource consumption. With the proposed model, we formulate the process of identifying the optimal design choice into an integer non-linear programming (INLP) problem and demonstrate that it can be solved efficiently with the help of a one-time C-to-HDL(hardware description language) synthesis. The experimental results show that our automated source-to-source code transformation tool improves the performance of a broad class of HLS designs by averagely 4.8x.\n","tags":["DAC"],"title":"Bandwidth Optimization Through On-Chip Memory Restructuring for HLS","type":"publication"},{"authors":["Chen Zhang","Zhenman Fang","Peipei Zhou","Peichen Pan","Jason Cong"],"categories":null,"content":"","date":1479380892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479380892,"objectID":"8d9d492fc557a37c4e291e1130b39b4c","permalink":"https://peipeizhou-eecs.github.io/publication/2016_iccad/","publishdate":"2016-11-17T11:08:12.2Z","relpermalink":"/publication/2016_iccad/","section":"publication","summary":"With the recent advancement of multilayer convolutional neural networks (CNN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy-efficiency of the computation-demanding CNN, the FPGA-based acceleration emerges as one of the most attractive alternatives. In this paper we design and implement Caffeine, a hardware/soft-ware co-designed library to efficiently accelerate the entire CNN on FPGAs. First, we propose a uniformed convolutional matrix-multiplication representation for both computation-intensive con-volutional layers and communication-intensive fully connected (FCN) layers. Second, we design Caffeine with the goal to maximize the underlying FPGA computing and bandwidth resource utilization , with a key focus on the bandwidth optimization by the memory access reorganization not studied in prior work. Moreover , we implement Caffeine in the portable high-level synthesis and provide various hardware/software definable parameters for user configurations. Finally, we also integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet network on multiple FPGA platforms. Caffeine achieves a peak performance of 365 GOPS on Xilinx KU060 FPGA and 636 GOPS on Virtex7 690t FPGA. This is the best published result to our best knowledge. We achieve more than 100x speedup on FCN layers over previous FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 7.3x and 43.5x performance and energy gains over Caffe on a 12-core Xeon server, and 1.5x better energy-efficiency over the GPU implementation on a medium-sized FPGA (KU060). Performance projections to a system with a high-end FPGA (Virtex7 690t) shows even higher gains.\n","tags":["ICCAD"],"title":"Caffeine: Towards Uniformed Representation and Acceleration for Deep Convolutional Neural Networks","type":"publication"},{"authors":["Peipei Zhou","Hyunseok Park","Zhenman Fang","Jason Cong","Andr√© DeHon"],"categories":null,"content":"","date":1463483351,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463483351,"objectID":"158c9f7932655f64a130d290c0922ca5","permalink":"https://peipeizhou-eecs.github.io/publication/2016_fccm/","publishdate":"2016-05-17T11:09:11.723Z","relpermalink":"/publication/2016_fccm/","section":"publication","summary":"Customized pipeline designs that minimize the pipeline initiation interval (II) maximize the throughput of FPGA accelerators designed with high-level synthesis (HLS). What is the impact of minimizing II on energy efficiency? Using a matrix-multiply accelerator, we show that matrix multiplies with II1 can sometimes reduce dynamic energy below II=1 due to interconnect savings, but II=1 always achieves energy close to the minimum. We also identify sources of inefficient mapping in the commercial tool flow. ","tags":["FCCM"],"title":"Energy Efficiency of Full Pipelining: A Case Study for Matrix Multiplication","type":"publication"},{"authors":["Yu-Ting Chen","Jason Cong","Zhenman Fang","Peipei Zhou"],"categories":null,"content":"","date":1455707410,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455707410,"objectID":"33aa904ca59fdd7816ec87159b216c21","permalink":"https://peipeizhou-eecs.github.io/publication/2016_fpga/","publishdate":"2016-02-17T11:10:10.25Z","relpermalink":"/publication/2016_fpga/","section":"publication","summary":"Compared to conventional general-purpose processors, accelerator-rich architectures (ARAs) can provide orders-of-magnitude performance and energy gains. In this paper we design and implement the ARAPrototyper to enable rapid design space explorations for ARAs in real silicons and reduce the tedious prototyping efforts. First, ARAPrototyper provides a reusable baseline prototype with a highly customizable memory system, including interconnect between accelerators and buffers, interconnect between buffers and last-level cache (LLC) or DRAM, coherency choice at LLC or DRAM, and address translation support. To provide more insights into performance analysis, ARAPrototyper adds several performance counters on the accelerator side and leverages existing performance counters on the CPU side. Second, ARAPrototyper provides a clean interface to quickly integrate a user?s own accelerators written in high-level synthesis (HLS) code. Then, an ARA prototype can be automatically generated and mapped to a Xilinx Zynq SoC. To quickly develop applications that run seamlessly on the ARA prototype, ARAPrototyper provides a system software stack and abstracts the accelerators as software libraries for application developers. Our results demonstrate that ARAPrototyper enables a wide range of design space explorations for ARAs at manageable prototyping efforts and 4,000 to 10,000X faster evaluation time than full-system simulations. We believe that ARAPrototyper can be an attractive alternative for ARA design and evaluation.","tags":["FPGA"],"title":"ARAPrototyper: Enabling Rapid Prototyping and Evaluation for Accelerator-Rich Architecture","type":"publication"},{"authors":["Yu-Ting Chen","Jason Cong","Jie Lei","Sen Li","Myron Peto","Paul Spellman","Peng Wei","Peipei Zhou"],"categories":null,"content":"","date":1439809861,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439809861,"objectID":"25c4eb18ecce8c7d14cb9bd78e5f1812","permalink":"https://peipeizhou-eecs.github.io/publication/2015_hitseq/","publishdate":"2015-08-17T11:11:01.653Z","relpermalink":"/publication/2015_hitseq/","section":"publication","summary":"The deep-coverage whole-genome sequencing (WGS) can generate billions of reads to be sequenced. It is time consuming for state-of-the-art aligners, such as BWA-MEM, to align the tremendous number of reads onto the reference genome. Inherently, the reads can be aligned using a massively parallel approach, and the alignment process should not be bounded by the limited number of computing cores of a single server. We present cloudscale BWAMEM (CS-BWAMEM), an ultrafast and highly scalable aligner built on top of cloud infrastructures. It leverages the abundant computing resources in a public or private cloud to fully exploit the parallelism obtained from the enormous number of reads. With CSBWAMEM, the pair-end whole-genome reads (30x) can be aligned within 80 minutes in a 25-node cluster with 300 cores.","tags":["HITSEQ"],"title":"CS-BWAMEM: A fast and scalable read aligner at the cloud scale for whole genome sequencing","type":"publication"},{"authors":["Peipei Zhou"],"categories":null,"content":"","date":1408274236,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408274236,"objectID":"277960f972b2339393b18fea6ae0f21b","permalink":"https://peipeizhou-eecs.github.io/publication/2014_master/","publishdate":"2014-08-17T11:17:16.204Z","relpermalink":"/publication/2014_master/","section":"publication","summary":"Future processor will not be limited by the transistor resources, but will be mainly constrained by energy efficiency. Reconfigurable architecture offers higher energy efficiency than CPUs through customized hardware and more flexibility than ASICs. FPGAs allow configurability at bit level to keep both efficiency and flexibility. However, in many computation-intensive applications, only word level customizations are necessary, which inspires coarse-grained reconfigurable arrays(CGRAs) to raise configurability to word level and to reduce configuration information, and to enable on-the-fly customization. Traditional CGRAs are designed in the era when transistor resources are scarce. Previous work in CGRAs share hardware resources among different operations via modulo scheduling and time multiplexing processing elements. In the emerging scenario where transistor resources are rich, we develop a novel CGRA architecture that features full pipelining and dynamic composition to improve energy efficiency and implement the prototype on Xilinx Virtex-6 FPGA board. Experiments show that fully pipelined and dynamically composable architecture(FPCA) can exploit the energy benefits of customization for user applications when the transistor resources are rich.\n","tags":["Thesis"],"title":"A Fully Pipelined and Dynamically Composable Architecture of CGRA (Coarse Grained Reconfigurable Architecture)","type":"publication"},{"authors":["Jason Cong","Hui Huang","Chiyuan Ma","Bingjun Xiao","Peipei Zhou"],"categories":null,"content":"","date":1400325137,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1400325137,"objectID":"9713b3a38591e8db745ec82901bb828b","permalink":"https://peipeizhou-eecs.github.io/publication/2014_fccm/","publishdate":"2014-05-17T11:12:17.735Z","relpermalink":"/publication/2014_fccm/","section":"publication","summary":"Future processor chips will not be limited by the transistor resources, but will be mainly constrained by energy efficiency. Reconfigurable fabrics bring higher energy efficiency than CPUs via customized hardware that adapts to user applications. Among different reconfigurable fabrics, coarse-grained reconfigurable arrays (CGRAs) can be even more efficient than fine-grained FPGAs when bit-level customization is not necessary in target applications. CGRAs were originally developed in the era when transistor resources were more critical than energy efficiency. Previous work shares hardware among different operations via modulo scheduling and time multiplexing of processing elements. In this work, we focus on an emerging scenario where transistor resources are rich. We develop a novel CGRA architecture that enables full pipelining and dynamic composition to improve energy efficiency by taking full advantage of abundant transistors. Several new design challenges are solved. We implement a prototype of the proposed architecture in a commodity FPGA chip for verification. Experiments show that our architecture can fully exploit the energy benefits of customization for user applications in the scenario of rich transistor resources.","tags":["FCCM"],"title":"A Fully Pipelined and Dynamically Composable Architecture of CGRA","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://peipeizhou-eecs.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://peipeizhou-eecs.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]